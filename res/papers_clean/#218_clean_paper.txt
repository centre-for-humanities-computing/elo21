transformative reading and writing synthetic archives with language models jonathan bradley gallagher university of colorado boulder intermedia art writing and performance may th abstract this paper reflects on electronic literature projects i created between and through interrogating how each project collaborates with an increasingly complex non human component riffing off of donna haraway s concept of significant otherness and making kin i speculate on the differences in the significance of the otherness that is engaged with in projects using methods based on combinatorics chance statistical models and vector semantics contemporary neural network based language models like gpt while recognizing that each approach involves a reduction in human agency this reflective paper focuses on the increasing complexity to which this agency is relinquished and how to deal with presenting this relationship between human and non human actors culminating in a series of projects using openai s gpt the need for a self reflexive transformative reading interface is introduced as a concrete instantiation of katherine hayles concept of a technotext a transformative reading interface links a corpus of text to text generated by a language model based on that corpus such an interface serves to provide a source of noisy creativity for writing and a way to explore the materiality of contemporary language models for reading while interrogating and respecting the posthuman nature of these artifacts in his book reading machines towards an algorithmic criticism stephen ramsay posits that any reading of a text that is not a recapitulation of that text relies on a heuristic of radical transformation this critical stance serves as foundational support for his assertion of the congruity between the narrowing constraints of computational logic the irreducible tendency of the computer toward enumeration measurement and verification and the goals of literary criticism while ramsay promulgates the merits of a computational natural language processing approach to literary criticism both in a way that compliments and augments traditional theory he is also quick to point out that this effort is not an attempt to reify literary ramsay stephen reading machines toward an algorithmic criticism university of illinois press p ibid criticism as finally relevant through being computationally defined he is also interested in the artistry involved in developing concrete implementations of algorithmic criticisms ramsay s aspirations in were admirable he imagined an approach to literary criticism that used the rigor of computer science to critically analyze text while using the rigor of literary criticism to interrogate how computational methods could be applied to reading writing and understanding literature in his closing remarks ramsay offers the following succinct and simple definition of algorithmic criticism as an attitude toward the relationship between mechanism and meaning that is expansive enough to imagine building as a form of thinking however in a review of the book susan brown points out that there is an odd tension between the insistence in this book on algorithmic criticism as offering a different scale and expanded powers of observation and the very contained examples ramsay provides comparing reading machines to moretti s graphs maps and trees brown says in advocating distant reading moretti makes a small number of large arguments in relation to the study of the novel ramsay makes a large number of small readings in order to mount a very big argument about the nature of texts and critical reading brown s points underscore the difficulty in expanding literary criticism through small algorithmic readings which in ramsay s account amounts to implementing many small natural language processing tasks on equally small and focused datasets not to mention the problematic expectation that a large number of humanities or even digital humanities scholars would want to engage in building as thinking at this low level the desire for a higher level of engagement of building as thinking is equally palpable in ramsay s vision as it is in brown s critical remarks ibid p brown susan review reading machines toward an algorithmic criticism literary and linguistic computing vol no pp since the writing of reading machines in significant progress has been made in applying artificial intelligence in the form of neural networks to classifying and generating text in november of openai released the full version of their transformer based model called gpt general pre trained transformer this type of neural network differs from previous architectures such as recurrent neural networks in that it consecutively builds connections between single words rather than working with sequences in text based neural networks words are represented as high dimensional vectors think lists of numbers often in the range of numbers per word called word embeddings and the training of the network could be thought of as finding likely paths through the vectors embedded in this high dimensional space in this paper i argue for exploring using gpt as a heuristic for transforming text and as a model for building as thinking that allows for a higher level of conceptual engagement i argue that this higher level of conceptual engagement emerges through collaborating with the nature of the vector space representation of words and how gpt s transformer based model builds connections within that space algorithmic collaboration of this kind involves a significant relinquishment of human agency as compared to ramsay s many building blocks of algorithmic criticism however it is important to understand what that algorithmic collaborator is and if one could should rely on it to do anything interesting in the space of algorithmic reading and writing this paper documents two creative experiments using open ai s gpt language model these playful experiments aim to explore the potential for using contemporary generative text systems as posthuman collaborators for developing creative and scholarly artifacts these https openai com blog tags gpt jurafsky dan and james h martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition nd ed pearson prentice hall ch artifacts seek to illuminate rather than obfuscate the materiality of these machine learning systems foregrounding the underlying technology with its strengths and limitations on full display while emphasizing the same for the role of human agency within the posthuman collaboration in this way these works are disinterested in the creative turing test and are aligned with the reasoning in the electronic book review article the anxiety of imitation on the boringness of creative turing tests by dan rockmore and kyle booten in their analysis of the hafez poetry generation system which involves both a procedural rule based system and the use of recurrent neural networks for classification rockmore and booten argue that the system itself should be considered a work of literary art not because of the mimetic merit of its output but that systems such as the hafez system are models of minds they may do things that remind us of how our minds use language and they may do things that seem utterly alien mechanical beyond the failure or success of an imitation game these models however imperfect they may be invite us into a recursive and unstable consideration of the ways that our most jealously guarded psychological attributes do or do not and should or should not could or could not carry the echo of an algorithm at the same time the authors note that the hafez system won the turing tests in the creative arts hosted by the neukom computational institute at dartmouth college this ironic nod of acclaim in an article dedicated to criticizing the imitation approach in creative ai artworks is indicative of the enchantment with an ethos of aesthetic appreciation surrounding ai generated works that is still ultimately entrenched in a liberal humanist perspective to counter this liberal humanist impulse these two projects embrace two operational tenets first i explore ways of comparing generated text to their corresponding training corpora ghazvininejad marjan et al hafez an interactive poetry generation system proceedings of acl system demonstrations association for computational linguistics pp aclweb https www aclweb org anthology p the anxiety of imitation on the boringness of creative turing tests electronic book review https electronicbookreview com essay the anxiety of imitation on the boringness of creative turing tests accessed dec ibid through employing natural language processing techniques and a bit of text processing dense sets of hyperlinks are created between generated and original corpora to provide a direct interface for comparing these respective texts these comparisons reveal both limitations and novel features of the generative text pipeline s behavior second building off of rockmore s and booten s idea of models of minds i explore designing generative text pipelines inspired by the narratives and relationships that exist between the authors of the training corpora this exploration is undertaken not to necessarily induce some sort of critical understanding between their works but to serve as frameworks to extend subvert and confound ramsay s notion of a hermeneutics of screwing around these extensions and subversions are transformative reading and writing with synthetic archives and noisy creativity concepts that will be elucidated on later in this paper language models and materiality in her book writing machines katherine hayles introduces the term technotext to describe a literary work that interrogates the inscription technology that produces it a technotext has the ability to mobilize reflexive loops between its imaginative world and the material apparatus embodying that creation as a physical presence the projects discussed in this paper proceed in this spirit and are in part aimed at identifying the attributes of language models and generative text systems that give rise to the salient features of their materiality while this viewpoint doesn t deny the embodied aspect of this materiality indeed special vectorized processors gpus which perform operations on lists of number simultaneously rather than ramsay stephen the hermeneutics of screwing around or what you do with a million books pastplay edited by kevin kee university of michigan press pp jstor doi j ctv swr hayles n katherine writing machines mit press p serially as on a traditional cpu are necessary for training and running inference with large neural network based language models i propose that the significant material feature of language models like gpt are the representations of word meanings learned using vector semantics an unsupervised machine learning technique that organizes these representations into a high dimensional vector space these word embeddings will have the feature that words with similar meanings are clustered around each other in this high dimensional space which can be visualized by projecting this space down to three or two dimensions to produce the word cloud visualizations that are often used in the digital humanities this technique is based on the distributional hypothesis which posits that words that have similar meanings tend to occur in similar contexts that there is a link between the similarity in the distribution of words and their meanings this view of materiality and language models is bolstered by katherine hayles analysis of the complex nature of materiality and physicality in her book how we think digital media and contemporary technogenesis hayles describes the evolution of human technical innovation as a manifestation of the cognitive skill of attention and its interaction with the interface between physicality and materiality for hayles the physicality of any object is essentially infinite and it is the role of human attention fussing with physicality to isolate some particular attribute or attributes of interest from which materiality emerges hayles also highlights the role of conceptual frameworks and methodological strategies that play an essential role in this emergence from this infinite array the various components of a computer in this case a technotext will select a few to foreground and work into its thematic concerns materiality thus jurafsky dan and james h martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition nd ed pearson prentice hall ch hayles n katherine how we think digital media and contemporary technogenesis the university of chicago press p emerges from interactions between physical properties and a work s artistic strategies hayles goes on to say that materiality depends on how the work mobilizes its resources as a physical artifact as well as on the user s interactions with the work and the interpretive strategies she develops strategies that include physical manipulations as well as conceptual frameworks therefore it is the focused attention in the form of a concept the distributional hypothesis that is instantiated through vector semantics a mathematical technique implemented in an algorithm that functions as the significant materiality with which a user can interact and an artist can make strategic decisions an attractive feature of this conception of the materiality of language models like gpt is how the emergence of the meaning of a word is essentially based on its relation to other words this synergizes well with the general trend in posthumanism to identify objects both inanimate and animate as relational complexes of agential components that cannot be trivially separated from the complex to which they belong this viewpoint doesn t deny the physical embodiment of the device that performs this computation but rather is pointing out that physicality i e the operation of the gpu would look the same whether it was processing text images or sound and that it is at a higher conceptual level where attention is being deployed in a particular way to bring to the foreground the attributes that give rise to the emergence of the materiality of a language model for the particular case of language models it is important to realize that the non human materiality that we are interacting with doesn t know anything at all about language grammar or word meanings in the sense that humans do instead we are making kin with non human word embeddings participating in a sympoietic process like that as described by beth dempster by way of donna haraway as collectively producing systems that do not have self defined spatial or temporal boundaries information and control are hayles n katherine writing machines mit press pp distributed among components the systems are evolutionary and have the potential for surprising change generative text systems and non human agency in order to set a foundation for how text generation with a language model like gpt operates at a different scale and provides innovative ways to read and write text i will first introduce two earlier projects that also function as text generation systems but in a less complex fashion playing off of donna haraway s idea of significant otherness i am arguing that the complexity of the potential interaction between a person and a language model like gpt is in and of itself significant because of the otherness of the conceptual space that is interacted with i e high dimensional word embeddings while this conceptual space is a far cry from what could be called conscious thinking or even intelligent i am arguing that it is complex enough and its details unknown enough in order for interaction with it to contain the same partial connections described in haraway s companion species manifesto patterns within which the players are neither wholes nor parts in this way the black box criticism of neural networks the idea that as these models grow bigger and bigger that it becomes harder and harder to actually understand exactly how they are learning what they are learning is turned on its head the black box characteristic of neural networks is the very quality that manifests the potential and opportunity to engage in a posthuman collaboration of significant otherness haraway donna jeanne staying with the trouble making kin in the chthulucene duke university press pg haraway donna jeanne the companion species manifesto dogs people and significant otherness prickly paradigm press pg the following projects are meant to illustrate an increasing gradient of complexity in the potential depth of significant otherness that is available to interact with based on the underlying materiality of the different text generation systems kinetic haiku generator the kinetic haiku generator is a webcam based project that incorporates computer vision and a d physics simulation engine called box d the kinetic haiku generator creates an environment where the user can see themselves and four colored circles that they can hit by intersecting their hand with the circles fig each circle is attached to a word type red noun blue verb green adjective yellow adverb and is assigned a number one through four that is its syllable count when a given circle is hit a random word is selected from a large dataset of words of a given type that have been organized by syllable count and printed to the screen provided the user hits the circles such that the five seven five syllable count for the three lines of the haiku are maintained a series of consonant tones are played if the user violates the syllable count for a given line by hitting a circle with a syllable count that puts it over the limit for that particular line a frequency modulation is applied to the tone while the piece requires the body to write and a focus of attention that oscillates between stress and flow through the bio feedback of the tones the text generation itself is purely combinatorial relying on pseudo random number generators for variation in this way its text generating capabilities are based on the same method and the outputs even more random and nonsensical due to the large word database size as allison knowles and james tenny s seminal computational poem house of dust while mostly relying on randomness the constraints of the haiku form and https nickm com memslam a_house_of_dust html link to an implementation of allison knowles and james tenny s creation the augmented or virtual form of embodiment that mediates the writing i e watching yourself hit virtual circles on a screen recalls the constraint based approaches of the ouilipian poets that extends the structures and patterns for the potential of literature into the realm of a kind of augmented reality fig kinetic haiku generator markov text editor another project i developed is the markov text editor this project incorporates n gram based language models and an ordinary text editor to create a tool that provides a noisy creativity for poetry generation a subtractive and additive sculptural approach of erasure and word addition employed to carve poems out of the large blocks computer generated text n gram language models are familiar additions to many commonly used programs and are responsible for text completion prediction functionality in applications such as text messaging an n gram model simply keeps the count of unique words or letters and the unique words or letters that follow a given word or letter in a given corpus so for instance in the following small corpus the dog chased the fox that chased the duck who bit the fox a gram model for the word the would be the dog the duck the fox meaning there is a chance that fox follows the and a chance for duck and dog to follow the once such a model has been built up from a large corpus of text the probability distribution of the model is sampled usually by using a pseudo random number generator in order to generate text claude shannon the founder of information theory was the first to describe this process known as shannon s game in his investigations with using n grams to compute approximations of english word sequences compared to the kinetic haiku generator the resulting output will be much more readable as only words or letters that appeared in the original corpus can possibly appear in the generated text in fact as the n gram model increases in size it will approach reproducing the corpus text verbatim however divergences from meaningful sentences do occur especially when you think of root words like the that have many possible different words that could potentially follow them in an effort to explore ways in which the agency of the machine could be extended in the markov text editor i experimented with using different distributions of numbers to sample the probability distributions of the n gram models some interesting cases are the effect of sampling an n gram probability distribution with a log normal distribution which i have found creates alliterative sequences of text another case is using the distribution of numbers that emerge out jurafsky dan and james h martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition nd ed pearson prentice hall ch of the logistic map a famous iterative equation from chaos theory this distribution of numbers oscillates between regularity and randomness sampling an n gram language model with this kind of distribution creates sequences of repetitive text followed by more heterogeneous text all of these techniques are examples of what i call noisy creativity sources of non random but noisy and novel patterns this is especially true when the n gram language model is applied to characters which allows for the spontaneous creation of new words see the poem southerine below fig fig a n gram language model of mary shelley s frankenstein sampled randomly fig b n gram model of mary shelley s frankenstein sampled with a log normal distribution notice the alliteration especially of a words southerine shit s outsiderably litterin with southerine with a love for this room to be anywhere else they ve been sick and shiftin sittin kitchen blowin smoke bitchin blind bad on silver sheen she lives entwines dilates lines of courtships down so start counting anywhere start today mornings i stay at the park across the street man my submarine afternoons i play to scare off to get off to take one off every mix their sweat rolls over they stop they piss stella says wave at puffy eyes for what water ricochets birds peck clean toilet flush and swirl powdered pearl bubbles drown these mean stares and streaks with a white clay pipe stella promise me flight that icky sticky heat let us go to the park across the street take pictures of your feet mount animal forms clench diagonal traces a flight back and forth before we eat i know this time buffeting below much trouble whistles but for now we are free from fight the red facade the leaves on tight the sounding wall what sinking sunlight twilight makes before sure hidden beat animals here shrinking seen choking cape escape it ape excise me unite this string of smoke there is no sound just an irregular heartbeat fake close your palms keep your eyes shut tight see the kudzu crop the swirl of dark can you feel it creep open your eyes look up do you see neverender s lake in a desert land me every autumn strand be stella lala lala boom a rang me speed me back and forth i know this time i jump off the swing fig poem carved out markov text editor output do gpt s dream of electric poetry gpt general pretrained transformer is known as an attention based neural network while the base model was trained on million web pages its attention based architecture makes it especially good at transfer learning this allows for fine tuning the large model on a much smaller corpus of text creating a new model that leverages the lower level features learned by the base model things such as likely word order apparent grammar etc while taking on the style and content of the fine tuning corpus other neural networks like traditional recurrent neural networks can be used to generate text but can also be used to classify it i e identify if a text was written by a certain author once trained on a subset of that author s corpus these experiments use both of these types of neural networks to create what i call hyper carving text generation pipelines the first gpt experiment discussed references to the title of philip k dick s book do androids dream of electric sheep not from any concern for whether or not gpt s trained on poetry dream of poetry or of anything at all they of course don t dream but with the general anxiety surrounding the social impact of the creative potential of artificial intelligence in dick s book the narrator is concerned with the social implications of owning an electric sheep in a world where animal ownership is a symbol of socio economic status the title creates a double ambiguity that creates a posthuman crisis of identity would androids be sophisticated enough to carry the shame of having an electric sheep in lieu of a real one as a status symbol or is the narrator actually an android and already in this absurd situation in this project i am building off the idea of carving text out of the output from n gram language models this concept was eloquently captured by david jhave johntson is his book aesthetic animisms a block of a i generated text massive and incomprehensible can exude the presence of solid stone here the cursor exists like a chisel i called this human editing part of the process carving hyper carving building off this concept i set out to create what i call a hyper carving pipeline that creates end to end computer generated text the design of this pipeline is informed by the social and critical relationship between the poets john ashbery w h auden and wallace stevens in a interview with david remnick john ashbery describes the formative impact that the poetry of w h auden had on his writing i am usually linked to wallace stevens but it seems to me auden played a greater role he was the first modern poet i was able to read with pleasure in another interview ashbery identifies auden as one of the writers who most formed my language as a poet for auden s part there was a mutual yet mysterious appreciation for the younger poet s work auden awarded ashbery the younger yale poets prize for his book of poems some trees with the caveat that he had not understood a word of it playfully building off this narrative i devised a text generation pipeline that involved fine tuning gpt models on the poetry of john ashbery and w h auden these two subsequently produced models were then put into conversation with each other the output of the ashbery model serves as an input prompt to the auden model and vice versa in a feedback loop the generated text is then classified with three separate recurrent neural networks one trained one the poetry of ashbery one trained on the poetry of auden and one trained on the poetry of johnston david jhave aesthetic animism digital poetry s ontological implications the mit press ashbery john remnick david september john ashbery in conversation with david remnick bennington review pattel cyrus r ed the cambridge history of american literature volume poetry and criticism pg orr d smith d sept rd john ashbery is dead at a poetic voice often echoed never matched the new york times https www nytimes com arts john ashbery dead prize winning poet html wallace stevens fig this classification part of the pipeline serves to filter the large amount of generated text into smaller subsets that have different characteristics for instance texts generated from the ashbery and auden models that have the highest score from the wallace trained rnn may have more novelty and outputs that have high scores from the rnns of the author they were trained on may be overfit one output of this process was a short partially machine curated and partially human curated list of poems that made use of these classification networks to find novel poems by searching for poems generated by the ashbery model for instance that had a higher auden score and an even higher stevens score a presentation of these poems can be seen here http jbgallag ddns net aa this sort of output is uncomfortably close to a creative turing test and while the success of the hyper carving method using recurrent neural networks trained on the different poets to filter a large amount of gpt output is a useful methodology to keep in the toolkit i was left unsatisfied with this type of output as a presentation of the process transformative reading and writing synthetic archives and noisy creativity this dissatisfaction led to the development of a concept to present the generative output of gpt models with the corpora that they were fine tuned on with such an approach it is possible to address practical matters such as whether or not the training was overfit which will reproduce the learned text verbatim and to explore the deeper interconnections between how the generated text and the original corpus are converging and diverging this is accomplished by fig a hyper carving pipeline for outside classification creating a dense set of hyperlinks between the generated text and the original corpus that links all occurrences of significant words between them these links can have multiple destinations i e a word in a generated text may link to multiple locations in the original training corpus and vice versa so the interface for traversing them cycles through these possibilities currently these transformative readings are presented in a web interface that allows for viewing generated text and the original corpus side by side with bi directional multi links between them in the case of the do gpt s dream of electric poetry project it is also possible to filter the generated output based on each generated poem s classification score for each of the three recurrent neural networks trained on ashbery auden and stevens the current version of this project can be interacted at the following links http jbgallag ddns net poemdata do gpt s dream of electric poetry http jbgallag ddns net gpc gnarly posthuman conversations john ashbery w h auden wallace stevens and gpt which is a new iteration of the project media archaeology language model in this project i am interested in exploring the potential of using the transformative reading interface as a creative tool for exploring scholarly texts the goal for the development of this system is to create a synthetic archive of generated text that is densely linked back to the original archive which is created by asking the fine tuned model questions and propagating the responses in the form of an exponential graph in the first iteration of this project around tokens of text related to the field of media archaeology see website below for references was used to fine tune a gpt model the subsequent model is prompted with a question which produces four responses the four responses are used as prompts to each generate four more responses etc up to a certain exponent in the current version of the process this is done five times which creates unique combinations of interconnected outputs of previous steps serving as prompts for subsequent steps responses to a single question this creates a nonlinear branching structure that is inspired by zielinski s deep time of the media his analogy between thomas hutton s discovery of unexpected strata of slate under layers of granite and the stratification of the history of media a deep time interpretation that resists a linear understanding of a predictable and necessary advance from primitive to complex apparatus the hope is that a transformative reading system like this can be used to build up a rich series of interconnected responses to questions that probe a given research topic which are in turn linked back to the original texts enabling an environment that contains a noisy creativity that can provide an innovative and novel way to approach creating critical discourse this could be a tool for creating a traditional paper by collaborating with such a system or simply by creating such a system itself zielinski siegfried deep time of the media toward an archaeology of hearing and seeing by technical means mit press pp the anti disciplinary spirit of media archaeology and its tendency to blend disparate disciplines i m thinking of zielinski s appropriation of geological methodology and thomas elsaesser s appropriation of the laws of thermodynamics to create a new and powerful way to think about the history of the cinema as poignant examples makes the field and its archive of discourse an apt partner to the techniques that can be employed with gpt in particular i have been interested in exploring a research project that looks at media archaeology through the lens of punctuated equilibrium an alternative theory of evolution put forth by stephen j gould that states that once a species emerges they often stay the same until they go extinct speciation on the other hand seems to occur in punctuated disruptive spurts that are often associated with the sub population of a species becoming geologically isolated from the main population i am interested in looking at the evolution of media technology through this methodology a transformative reading environment to support this could involve creating separate gpt models one based on media archaeology scholarship and the other on scholarship concerning punctuated equilibrium and putting them into conversation with each other as with the poetry project another idea would be to train a gpt model first on media archaeology scholarship and then train the same model again in a transfer learning approach on the punctuated equilibrium material to create a blended model in further iterations of this project i am interested in extracting the questions that have happened to occur in the generated text and adding them as prompts to the process allowing for a natural source of feedback that allows gpt to directly change the course of the synthetic archive malm http jbgallag ddns net malm roberts ben and mark goodall new media archaeologies amsterdam univ press elsaesser thomas cinema motion energy and entropy pp conclusion the projects surveyed in this paper represent small steps towards developing new ways of reading writing and interacting with text using language models building off of stephen ramsay s idea of reading as a type of transformation i have begun an investigation in using generative language models like gpt to create environments of transformative reading where synthetic archives of text and the corpora they are trained on can be viewed together to be scrutinized explored and appreciated as works that are the result of posthuman collaboration i m arguing for the use of neural network based language models over the sort of nuts and bolts natural language processing techniques promulgated by ramsay in reading machines because they afford us a way to work in a conceptual space we can create conversations between dead poets and create synthetic archives of exponential answers to questions in a scholarly field and produce this text at scale in zielinski s investigation into the deep time of media he describes the type of media history he is interested in as curiosities by curiosities i mean finds from the rich history of seeing hearing and combining using technical means things in which something sparks or glitters their bioluminescence and also points beyond the meaning or function of their immediate context of origin as a prompt zielinski s quote engages me to respond with a couple of questions as language models continue to evolve openai has already developed a gpt model that is times bigger than gpt could the spirit of zielinski s curiosities be embodied in the development of synthetic histories on which a simulated discourse is performed could the resultant synthetic discourse provide anything useful for understanding the past and the future from a posthuman perspective zielinski siegfried deep time of the media toward an archaeology of hearing and seeing by technical means mit press pp annotated bibliography ramsay stephen reading machines toward an algorithmic criticism university of illinois press reading machines provides a great set of examples of how natural language processing techniques can be applied to critical discourse to perform what he calls algorithmic criticism his detailed approach pays homage to the artistry behind the design of such systems but still presents a rather low level approach that while he successfully argues is connected to traditional critical analysis seems unlikely to be widely practiced for my purposes his idea of all reading being a heuristic of transformation and the idea of building as a type of thinking were fantastic jumping off points for my arguments about the higher level type of building as thinking that can be done with contemporary transformer language models like gpt brown susan review reading machines toward an algorithmic criticism literary and linguistic computing vol no pp this review was useful for situating the difficulty in the adoption of ramsay s techniques and helped establish the notion of a desire for a higher conceptual level of interaction within algorithmic criticism https openai com blog tags gpt this is the main site for the gpt language model the language model uses billion parameters these are the weights single numbers in the end whose particular values are the result of the training of the initial model and the fine tuning process these weights describe the cumulative prediction of what the next word in a sequence might be in this way they describe likely paths through the vector space representation of the words in a corpus jurafsky dan and james h martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition nd ed pearson prentice hall this book provides a comprehensive overview of natural language processing both from a technical perspective and with great examples of underlying linguistic motivations for various nlp techniques the book covers everything up to recurrent neural networks in this paper i relied on it primarily for its section on vector semantics and word embeddings the link between word embeddings and neural networks is important because these serve as the input to the neural network and it is the underlying structure in the word embeddings that is being learned in an unsupervised way in the case of gpt all the other operations of a neural network the feed forward calculations and back propagation methods that adjust the weights are pretty similar for any type of neural network the motivation for using word embeddings is based on the distributional hypothesis which became an important concept for describing the important aspect of materiality in these language models from the perspective of this paper ghazvininejad marjan et al hafez an interactive poetry generation system proceedings of acl system demonstrations association for computational linguistics pp aclweb https www aclweb org anthology p this paper describes the hafez poetry system which is the canonical example of a hyper carving pipeline as far as i know the system generates sonnets through a partly procedural and partly machine learning based pipeline sonnets are constructed procedural through databases connecting rhyming words and rules are applied to enforce iambic pentameter the resulting poems being largely combinatorially created are often nonsensical a recurrent neural network trained on song lyrics and other types of corpora is used to filter out these nonsensical outputs ramsay stephen the hermeneutics of screwing around or what you do with a million books pastplay edited by kevin kee university of michigan press pp jstor doi j ctv swr while ramsay describes a hermeneutics of screwing around as a valid way to deal with a million books one of the questions i am asking in this paper is what it means to extend that hermeneutics to creating synthetic archives and works of literature which could vastly exacerbate the situation ramsay is describing in this essay hayles n katherine writing machines mit press i take hayles idea of technotext as the inspiration for the transformative reading interface i develop in these projects these projects are an initial attempt to instantiate the idea of a literary work that interrogates the inscription process that produces it and mobilizes reflexive loops between its imaginative world and the material apparatus embodying that creation as a physical presence hayles n katherine how we think digital media and contemporary technogenesis the university of chicago press hayles complex description of materiality one that relies on both physicality and the focusing of attention to create the emergent phenomena of materiality was essential in trying to decide where the important aspect of materiality in language models like gpt lies hayles allowing for conceptual frameworks and artistic strategies as aspects of that focusing of attention gave me the permission to look at the distributional hypothesis in the form of vector word embeddings as the important aspect of materiality that emerges in the way hayles describes gpt is a neural network and as such it has a large amount of weights that are adjusted through the training process which often involves a feed forward phase and back propagation phase which are fairly ubiquitous to any type of neural network it is the word embeddings which serve as the input into models like gpt where the emergence of materiality occurs through the focusing of attention on a particular virtual physicality haraway donna jeanne staying with the trouble making kin in the chthulucene duke university press haraway s development of sympoiesis helps situate the type of relationship and context of interpretation that i see existing in the interaction with language models like gpt haraway donna jeanne the companion species manifesto dogs people and significant otherness prickly paradigm press i riff off of haraway s idea of significant otherness drifting ever so slightly away from the sense of the word significant as i believe she intended through surveying two other text generation projects that i did in the past that did not use neural networks or word embedding models of words i attempt to demonstrate that the significance of the otherness that i interact with in both the kinetic haiku generator and the markov text editor is of a lesser value or level of complexity for potential interaction as compared to gpt whose nexus of material interaction is the black box of a neural network https nickm com memslam a_house_of_dust html link to an implementation of allison knowles and james tenny s creation included as the quintessential example of a combinatorial randomly sampled generated text system johnston david jhave aesthetic animism digital poetry s ontological implications the mit press jhave s quote is important for situating the model in which algorithmically assisted writing has generally followed in the hypercarving approach this view of the late night sculptor is excised this view of carving still places the human as the final designer and augmenter of reality and is still overly drenched in liberal humanism zielinski siegfried deep time of the media toward an archaeology of hearing and seeing by technical means mit press zielinski s ideas of the deep time of media and curiosities help situate why such an enterprise as transformative reading and writing with synthetic archive may have any value at all as zielinski hopes to find curiosities innovations and moments of time that history has looked over the process of generating text with an advanced language model like gpt and the techniques of dealing with that output could possibly led to a system that can be used to simulate the sort of fissured stratified and buried histories that zielinski is interested in develop methods for uncovering roberts ben and mark goodall new media archaeologies amsterdam univ press elsaesser thomas cinema motion energy and entropy pp thomas elsaesser s cinema motion energy and entropy the laws of thermodynamics are blended with the media theory and history of the cinema thinking of the cinema in terms of energy exchanges both in its physical operation in the images it depicts in the physiological stimulation of the viewer to the nineteenth century preoccupation with maximizing the energy of human labor a powerful language for elsaesser emerges that allows him to conceive of cinema as a discourse engaged in training the body and the senses in such a way that we experience as entertaining what society requires from us as a necessity in order for its particular form of energy transformation in this case capitalist production methods to function 